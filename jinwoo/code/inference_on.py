import os
import io
import torch
import numpy as np
import ffmpeg
import soundfile as sf
def pad_audio_to_target_length(audio, target_length, sampling_rate):
    """ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ Whisper ëª¨ë¸ì˜ ê¸°ëŒ€ ê¸¸ì´(30ì´ˆ)ë¡œ íŒ¨ë”©í•˜ëŠ” í•¨ìˆ˜"""
    current_length = len(audio)
    target_samples = target_length * sampling_rate  # 30ì´ˆ ê¸¸ì´ì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œ ìˆ˜

    if current_length < target_samples:
        pad_width = target_samples - current_length
        audio = np.pad(audio, (0, pad_width), mode='constant', constant_values=0)

    return audio
def load_audio_wav(file_path):
    """
    soundfileì„ ì´ìš©í•´ .wav íŒŒì¼ì„ numpy ë°°ì—´ê³¼ ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    audio, sr = sf.read(file_path)
    if audio.ndim > 1:
        audio = np.mean(audio, axis=1)  # ë‹¤ì±„ë„ ì˜¤ë””ì˜¤ë¥¼ monoë¡œ ë³€í™˜
    return audio, sr
def load_audio_ffmpeg(file_path, target_sr=16000):
    """
    ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ mp4 íŒŒì¼ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ wav í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ê³ ,
    soundfileì„ ì´ìš©í•´ numpy ë°°ì—´ê³¼ ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # ffmpegë¡œ ì˜¤ë””ì˜¤ë¥¼ wav í˜•ì‹ìœ¼ë¡œ íŒŒì´í”„ ì¶œë ¥, monoë¡œ ë³€í™˜
        out, err = (
            ffmpeg
            .input(file_path)
            .output('pipe:', format='wav', acodec='pcm_s16le', ac=1, ar=target_sr)
            .run(capture_stdout=True, capture_stderr=True)
        )
    except ffmpeg.Error as e:
        print("FFmpeg error:", e)
        raise e

    # ë©”ëª¨ë¦¬ì—ì„œ wav ë°ì´í„°ë¥¼ ì½ì–´ numpy arrayë¡œ ë³€í™˜
    audio, sr = sf.read(io.BytesIO(out))
    return audio, sr
def inference_on_wav(file_path, model, processor, segment_length_sec=30):
    sampling_rate = 16000

    # .wav íŒŒì¼ ë¡œë“œ
    audio, sr = load_audio_wav(file_path)
    if sr != sampling_rate:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ìƒ˜í”Œë§ ë ˆì´íŠ¸: {sr}. {sampling_rate} Hzë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    if audio.ndim > 1:
        audio = np.mean(audio, axis=0)  # ë‹¤ì±„ë„ -> ëª¨ë…¸ ë³€í™˜

    num_samples = len(audio)
    segment_samples = segment_length_sec * sampling_rate
    transcripts = []

    print("ì „ì²´ ì˜¤ë””ì˜¤ ê¸¸ì´(ìƒ˜í”Œ):", num_samples)
    print("ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìƒ˜í”Œ ìˆ˜:", segment_samples)

    # 2) forced_decoder_ids (ì–¸ì–´/íƒœìŠ¤í¬ íƒœê·¸)
    forced_decoder_ids = processor.get_decoder_prompt_ids(language="ko", task="transcribe")
    # í˜¹ì‹œ None/ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ model.config.forced_decoder_idsë¡œ ëŒ€ì²´í•˜ê±°ë‚˜, ë¹ˆ ë¦¬ìŠ¤íŠ¸ í• ë‹¹
    if not forced_decoder_ids:
        forced_decoder_ids = model.config.forced_decoder_ids
        if not forced_decoder_ids:
            forced_decoder_ids = []

    # 3) Whisper íƒ€ì„ìŠ¤íƒ¬í”„ ì„¤ì •
    #   (a) no_timestamps_token_id í™•ì¸
    if not hasattr(model.config, "no_timestamps_token_id"):
        no_ts_token = getattr(processor.tokenizer, "no_timestamps_token", "<|notimestamps|>")
        model.config.no_timestamps_token_id = processor.tokenizer.convert_tokens_to_ids(no_ts_token)

    #   (b) ì²« íƒ€ì„ìŠ¤íƒ¬í”„ ì¸ë±ìŠ¤ ì œí•œ (ê¸°ë³¸ 1, ë” ëŠ˜ë ¤ë„ ë¨)
    model.config.max_initial_timestamp_index = 50

    #   (c) suppress_tokens=None or íŠ¹ìˆ˜ í† í° ì œê±°
    #       í•„ìš”í•˜ë‹¤ë©´ model.config.suppress_tokens = [] ë¡œ ë¹„ìš¸ ìˆ˜ë„ ìˆìŒ.
    #       (ê¸°ì¡´ WhisperëŠ” íŠ¹ì • í† í°ì„ ì–µì œí•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
    # model.config.suppress_tokens = []

    #   (d) ë””ì½”ë”© ê¸¸ì´ ì—¬ìœ (ê¸°ë³¸ê°’ 448ì¼ ìˆ˜ ìˆëŠ”ë°, ë¶€ì¡±í•˜ë©´ ëŠ˜ë ¤ë³¼ ìˆ˜ ìˆìŒ)
    # model.config.max_length = 1024  # ë˜ëŠ” generate()ì—ì„œ max_new_tokens=... ì§€ì •

    # 4) ì„¸ê·¸ë¨¼íŠ¸ë³„ ì¶”ë¡ 
    for start in range(0, num_samples, segment_samples):
        end = min(start + segment_samples, num_samples)
        segment = audio[start:end]

        # ì…ë ¥
        inputs = processor(segment, sampling_rate=sampling_rate, return_tensors="pt")
        input_features = inputs["input_features"].to(model.device)

        with torch.no_grad():
            # âœ… ì—¬ê¸°ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ í™œì„±í™”ë¥¼ ìœ„í•´:
            #   return_timestamps=True + return_dict_in_generate=True
            outputs = model.generate(
                input_features,
                forced_decoder_ids=forced_decoder_ids,
                # suppress_tokens=None,
                return_dict_in_generate=True,
                return_timestamps=True,
                # max_new_tokens=1024  # í•„ìš”ì‹œ ì¶”ê°€
            )

        # 5) íƒ€ì„ìŠ¤íƒ¬í”„ ëª¨ë“œì¼ ë•Œ, WhisperForConditionalGenerationì€
        #    outputs.sequences + outputs.segments ë¥¼ ë°˜í™˜(ê°€ëŠ¥)
        if hasattr(outputs, "segments") and outputs.segments:
            # *ë¬¸ì¥ë³„* (ì„¸ê·¸ë¨¼íŠ¸ ë‚´) íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë”°ë¡œ ì¶”ì¶œ
            seg_transcripts = []
            for seginfo in outputs.segments:
                seg_text = seginfo["text"]
                seg_transcripts.append(seg_text)
            segment_text = " ".join(seg_transcripts)
        else:
            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì „í˜€ ì•ˆ ë‚˜ì˜¤ë©´, fallbackìœ¼ë¡œ ê·¸ëƒ¥ ë””ì½”ë”©
            sequences = outputs.sequences
            segment_text = processor.tokenizer.decode(sequences[0], skip_special_tokens=True)

        transcripts.append(segment_text)
        print(f"Segment {start // segment_samples + 1} ì™„ë£Œ. í…ìŠ¤íŠ¸: {segment_text}")

    full_transcription = "\n".join(transcripts)
    return full_transcription

def inference_on_file(file_path, model, processor, segment_length_sec=30):
    """
    ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ íŒŒì¼(MP4/WAV)ì„ Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    sampling_rate = 16000
    audio, sr = load_audio_ffmpeg(file_path, target_sr=sampling_rate)
    if audio.ndim > 1:
        audio = np.mean(audio, axis=0)  # ë‹¤ì±„ë„ -> ëª¨ë…¸ ë³€í™˜

    num_samples = len(audio)
    segment_samples = segment_length_sec * sampling_rate
    full_transcript = []

    print("ì „ì²´ ì˜¤ë””ì˜¤ ê¸¸ì´(ìƒ˜í”Œ):", num_samples)
    print("ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìƒ˜í”Œ ìˆ˜:", segment_samples)

    # ëª¨ë¸ì— ê°•ì œ ë””ì½”ë”© ì„¤ì • ì ìš© (í•œê¸€ë¡œ ì„¤ì •)
    forced_decoder_ids = processor.get_decoder_prompt_ids(language="ko", task="transcribe")
    print("forced_decoder_ids:", forced_decoder_ids)  # ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë‚´ìš© í™•ì¸
    # 2) Noneì´ê±°ë‚˜ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ìµœì†Œí•œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ì§€ì • (ì—ëŸ¬ ë°©ì§€)
    if not forced_decoder_ids:
        forced_decoder_ids = []
    model.generation_config.forced_decoder_ids = forced_decoder_ids
    model.config.forced_decoder_ids = forced_decoder_ids
    # ëª¨ë¸ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ê´€ë ¨ ì„¤ì •ì´ ì—†ì„ ê²½ìš° ì¶”ê°€
    if not hasattr(model.generation_config, "no_timestamps_token_id"):
        no_ts_token = getattr(processor.tokenizer, "no_timestamps_token", "<|notimestamps|>")
        model.generation_config.no_timestamps_token_id = processor.tokenizer.convert_tokens_to_ids(no_ts_token)
    model.generation_config.max_initial_timestamp_index = 10

    for start in range(0, num_samples, segment_samples):
        end = min(start + segment_samples, num_samples)
        segment = audio[start:end]
        offset = start / sampling_rate  # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹œì‘ ì‹œê°„(ì´ˆ)


        if len(segment) < segment_samples:
            segment = pad_audio_to_target_length(segment, segment_length_sec, sampling_rate)


        # ì…ë ¥ ìƒì„±
        inputs = processor(segment, sampling_rate=sampling_rate, return_tensors="pt", padding=True)
        input_features = inputs["input_features"].to(model.device)

        # Whisper ëª¨ë¸ì„ í†µí•œ ì¶”ë¡  ìˆ˜í–‰ (íƒ€ì„ìŠ¤íƒ¬í”„ í™œì„±í™”)
        with torch.no_grad():
            outputs = model.generate(
                input_features,

                return_dict_in_generate=True,
                return_timestamps=True
            )
            
                # ğŸš¨ ë””ë²„ê¹…: ëª¨ë¸ ì¶œë ¥ í™•ì¸
        print("ğŸ”¹ ëª¨ë¸ ì¶œë ¥:", outputs)

        if hasattr(outputs, "segments"):
            print(f"ğŸ”¹ ì¶œë ¥ëœ ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜: {len(outputs.segments)}")
        else:
            print("âŒ `outputs.segments`ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ!")

        # íƒ€ì„ìŠ¤íƒ¬í”„ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if hasattr(outputs, "segments") and outputs.segments:
            for seg in outputs.segments:
                start_time = seg['start'] + offset
                end_time = seg['end'] + offset
                transcript = f"[{start_time:.2f}s - {end_time:.2f}s] {seg['text']}"
                full_transcript.append(transcript)

        print(f"Segment {start // segment_samples + 1} ì™„ë£Œ.")

    # ì „ì²´ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
    full_transcription = "\n".join(full_transcript)
    return full_transcription